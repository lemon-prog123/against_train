import numpy as np
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
from re_utils import Conv
from re_utils import Linear
class ConvNet(nn.Module):
    def __init__(self, **kwargs):
        super(ConvNet, self).__init__()
        self.conv1 = Conv(3, 15,3)     # è¾“å…¥3é€šé“ï¼Œè¾“å‡?15é€šé“ï¼Œå·ç§¯æ ¸ä¸?3*3
        self.conv2 = Conv(15, 75,4)    # è¾“å…¥15é€šé“ï¼Œè¾“å‡?75é€šé“ï¼Œå·ç§¯æ ¸ä¸?4*4
        self.conv3 = Conv(75,375,3)    # è¾“å…¥75é€šé“ï¼Œè¾“å‡?375é€šé“ï¼Œå·ç§¯æ ¸ä¸?3*3
        self.fc1 = Linear(1500,400)       # è¾“å…¥2000ï¼Œè¾“å‡?400
        self.fc2 = Linear(400,120)        # è¾“å…¥400ï¼Œè¾“å‡?120
        self.fc3 = Linear(120, 84)        # è¾“å…¥120ï¼Œè¾“å‡?84
        self.fc4 = Linear(84, 10)         # è¾“å…¥ 84ï¼Œè¾“å‡? 10ï¼ˆåˆ†10ç±»ï¼‰
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    def regularizationTerm(self,beta=1e-2,mode=0):
        loss=torch.zeros(1).to(self.device)
        if mode==0:
            for layer in self.modules():
                if type(layer)==Linear:
                    w=layer._parameters['weight']
                    m=w@w.T
                    loss+=torch.norm(m-torch.eye(m.shape[0]))
                if type(layer)==Conv:
                    w=layer._parameters['weight']
                    N,C,H,W=w.shape
                    w=w.view(N*C,H,W)
                    m=torch.bmm(w,w.permute(0,2,1))
                    loss+=torch.norm(m-torch.eye(H))
        else:
            for layer in self.modules():
                if type(layer)==Linear:
                    iteration=1
                    w = layer._parameters[ 'weight' ]
                    v=layer.v
                    u=torch.normal(0,1,size=(w.shape[0],1)).cuda()
                    for _ in range(iteration):
                         u=torch.nn.functional.normalize(torch.mm(w.detach(),v).cuda(),dim=0)
                         v=torch.nn.functional.normalize(torch.mm(w.detach().T,u).cuda(),dim=0)
                    layer.v= v
                    loss+=(u.T@w@v).view(1)
                elif type(layer)==Conv:
                    iteration =1
                    w = layer._parameters[ 'weight' ]
                    v=layer.v
                    N, C, H, W = w.shape
                    m=w.view(N,C*W*H)
                    u=torch.normal(0,1,size=(m.shape[0],1)).cuda()
                    for _ in range(iteration):
                         u=torch.nn.functional.normalize(torch.mm(m.detach(),v).cuda(),dim=0)
                         v=torch.nn.functional.normalize(torch.mm(m.detach().T,u).cuda(),dim=0)
                    layer.v = v
                    loss+=(u.T@m@v).view(1)
        return beta*loss



    def forward(self, x,mode=0):
        """
            x: è¾“å…¥å›¾ç‰‡
            quant: æ˜¯å¦ä½¿ç”¨æ¨¡å‹é‡åŒ–
        """
        x = F.max_pool2d(F.relu(self.conv1(x)), 2)  # 3*32*32  -> 150*30*30  -> 15*15*15
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)  # 15*15*15 -> 75*12*12  -> 75*6*6
        x = F.max_pool2d(F.relu(self.conv3(x)), 2)  # 75*6*6   -> 375*4*4   -> 375*2*2
        x = x.view(x.size()[0], -1)  # å°?375*2*2çš„tensoræ‰“å¹³æˆ?1ç»´ï¼Œ1500
        x = F.relu(self.fc1(x))  # å…¨è¿æ¥å±‚ 1500 -> 400
        x = F.relu(self.fc2(x))  # å…¨è¿æ¥å±‚ 400 -> 120
        x = F.relu(self.fc3(x))  # å…¨è¿æ¥å±‚ 120 -> 84
        x = self.fc4(x)  # å…¨è¿æ¥å±‚ 84  -> 10
        return x
